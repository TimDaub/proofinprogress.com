# The LLM Bubble Is Boiling Our Oceans

With this article, I want to outline my current thinking around large language
models like OpenAI's chatGPT. As many smart people have made claims in public
about the potential of this technology, in the following, I'm going to make my
claims in public too so that I might be proven right or wrong later.

### i.

The hallmark of a bubble are its investors' inflated expectations around what
said technology can achieve. Researcher Carlota Perez realized this by
identifying five bursts of technological advancements. What's notable about her
list of bubbles including, (1) the industrial revolution, (2) the steam engine,
(3) the age of electricity, (4) the oil boom and (5) the dawn of the
information revolution, is that none of these technologies are fads. A bubble's
hallmark, hence, isn't the hyped-up technology's potential - it's the
investors' inflated expectations. It's exemplified in a recent quote of Marc
Andreessen remarking that (in tech) "timing is everything," with him hinting at
the eventual viability of failed dotcom startup ideas now producing big
revenues as successor companys as buying petfood online is now obviously
possible. So while LLMs promise and deliver a big potential for the direction
of AI, with investor's false expectations today, the public discouse reached
the peak of the Gartner Hype Cycle.

In deceiving the masses, social media played a significant but innocent role as
the promoters of a large logical error, an enormous survivorship bias. When
ChatGPT entered the zeitgeist a month ago, content aggregators like Twitter,
bubbled up the engaging conversations of users and the model. But only those
that users liked and retweeted circulated. Which contributed to a strange
spread between a user's expecation and experience when first talking to the AI.
While the experience truly lived up to the hype: Claims of Twitter users who
said that ChatGPT implemented entire front end turned out to be fake. With
Twitter's engagement algorithm promoting outrageous AI dialogues its impossible
to get an overview over users' true experiences. We're experiencing
survivorship bias as we're interpreting the ingenuity of ChatGPT through
engaging Twitter posts when there are thousands of unliked conversations
uploaded every second. We know that the expectations are inflated when we
evaluate our peer's opinions on large language models with our own experiences.

Upsetting to OpenAI customers is the revelation of ChatGPT's limited context
length. While it fools the average Twitter user, 4000 tokens or roughly 3000
words (8 pages in a book, 12pt font), ChatGPT is not smart enough to summarize
larger amounts of text to save the user time. Although it was presented as
capable of replacing front end developers, it's clear that this won't happen
soon as increasing context size will yield more costs for OpenAI.

### ii.

Last years biggest story in environmental-friendly computation was arguably the
crypto currency Ethereum's move from Proof of Work to Proof of Stake. With the
Merge executing on September 15, 2022, its energy consumption was reduced by
roughly 99.95% adjusting ETH's trajectory from an ocean-boiling technology to
an energy-independent financial substrate. One can speculate, however, that the
GPUs deprecated in Ethereum's Merge found good use in their new owner's data
centers.

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">interesting to me how many of the ChatGPT takes are either &quot;this is AGI&quot; (obviously not close, lol) or &quot;this approach can&#39;t really go that much further&quot;.<br><br>trust the exponential. flat looking backwards, vertical looking forwards.</p>&mdash; Sam Altman (@sama) <a href="https://twitter.com/sama/status/1599111626191294464?ref_src=twsrc%5Etfw">December 3, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Large language models, measured by the amount of their ingested
tokens, number of parameters and consumed computation now represent an entire
industry with each of big tech having a horse in the race. With all of them
specifying their models based on the above criteria, it has become an ML
sleuth's favorite to compute back-of-the-envelope calculations on what training
these models costs.

## References:

- https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0
- https://help.openai.com/en/articles/6787051-does-chatgpt-remember-what-happened-earlier-in-the-conversation
- https://docs.google.com/spreadsheets/d/1CLRNO2ZauecthThOiPI59LaenfWn4EwHdhffLdHu2s8/edit?usp=sharing
