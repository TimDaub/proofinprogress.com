
<!doctype html>
<html>
  <head>
    <script defer data-domain="proofinprogress.com" src="https://plausible.io/js/script.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta charset="utf-8"/>
    <meta property="og:title" content="The AI Crowd is Mad" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://proofinprogress.com/posts/2023-01-24/the-ai-crow-is-mad.html" />
    <meta property="og:image" content="https://proofinprogress.com/assets/images/soyfacing.jpg" />
    <meta property="article:author" content="Tim DaubenschÃ¼tz">
    <meta property="article:published_time" content="2023-01-24T00:00:00.000Z">
    <meta property="og:site_name" content="Proof In Progress">
    <meta name="description" content="With this write-up, I want to outline my current thinking around large language
models like OpenAI&apos;s chatGPT.">
    <meta name="twitter:title" content="The AI Crowd is Mad">
    <meta name="twitter:description" content="With this write-up, I want to outline my current thinking around large language
models like OpenAI&apos;s chatGPT.">
    <meta property="twitter:image" content="https://proofinprogress.com/assets/images/soyfacing.jpg" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@timdaub">
    <title>The AI Crowd is Mad</title>

    <link rel="stylesheet" href="/katex.min.css">
    
    <style>
      html {
        background-color: #FAF9F6;
        overflow-wrap: break-word;
        max-width: 70ch;
        padding: calc(1vmin + .5rem);
        margin-inline: auto;
        font-size: clamp(1em, 0.909em + 0.45vmin, 1.25em);
        font-family: system-ui;
        color: #444;
      }

      table {
        border-collapse: collapse;
        margin: 0;
        padding: 0;
        width: 100%;
        table-layout: fixed;
      }

      tr:nth-child(even), th {
        background: #F0F0F0;
      }

      table td, table th {
        padding: .325em;
        text-align: center;
      }

      body :not(:is(h1,h2,h3,h4,h5,h6)) {
        line-height: 1.5;
      }

      blockquote {
        font-style: italic;
      }

      p {
        text-align: justify;
        hyphens: auto;
      }

      img {
        width: 100%;
      }

      h1,
      h2,
      h3,
      h4,
      h5,
      strong {
        color: #222;
      }

      ul {
        list-style-type: square;
      }

      ul, ol {
        padding-left: 30px;
      }

      pre {
        overflow: auto;
        background-color: rgba(0,0,0,0.01);
        padding: 10px;
      }
    </style>
  </head>
  <body>
    <a href="https://proofinprogress.com">proofinprogress.com</a>
		<br>
    <hr>
    <h1>The AI Crowd is Mad</h1>
<p>With this write-up, I want to outline my current thinking around large language
models like OpenAI's chatGPT.</p>
<p>As many smart people have made claims about the potential of this technology,
in the following, I'm going to make my claims too, so that I might be proven
right or wrong later.</p>
<p>In general, I think the current LLM discourse needs more nuance. On podcasts
and in blog posts I always seem to identify one line of reasoning and it is too
optimistic for my taste.</p>
<h3>i.</h3>
<p>The hallmark of a bubble are its investors' inflated expectations given what
said technology can achieve. This becomes clear when enumerating Carlota
Perez's list of technological advancements and their ensuing market manias. It
consists of (1) the industrial revolution, (2) the steam engine, (3) the age of
electricity, (4) the oil boom and (5) the dawn of the information revolution.</p>
<p>In hindsight, clearly none of the five technologies are fads. Instead, the
crowd was mad. A certain bubble's hallmark, hence, isn't the technology's
potential - it's the investors' inflated expectations.</p>
<p>In recent history, this is exemplified in a quote from Marc Andreessen, saying
that, &quot;[in tech] timing is everything,&quot; with him alluding to the observation
that we can now buy dog food online, despite pets.com having been the tech
industry's laughing stock for two decades.</p>
<img alt="Soyfacing meme" style="display: block; margin: 0 auto; width: 40%;" src="../../assets/images/soyfacing.jpg" />
<p>So while LLMs are furthering our dreams of what AI could become one day, with
investor's false expectations today, the public discourse reached the peak of
the Gartner's Hype Cycle.</p>
<h3>ii.</h3>
<p>In deceiving the masses, social media played an important role as the promoter
of a large logical error, that of an enormous survivorship bias.</p>
<p>When ChatGPT entered the timelines a month ago, content aggregators like
Twitter, bubbled up the most engaging conversations of users and the model.</p>
<p>But only those that users liked and retweeted ended up circulating, which
contributed to a strange spread between a users' expecations and experiences
they had when first talking to chatGPT.</p>
<p>While personally, the experience lived up to some of the hype, social media
claims of it implementing entire front ends truned out to be fake. Clearly,
this models performs well on in-sample tasks, and is very far off on anything
else.</p>
<p>But with Twitter's engagement algorithm optimized for promoting the most
outrageous AI conversations, it is not possible to get an reasonable overview
of chatGPT's actual performance.</p>
<img alt="Survivorship Bias" style="display: block; margin: 0 auto; width: 40%;" src="../../assets/images/survivorship-bias.png" />
<p>We're collectively experiencing survivorship bias as we are drawn to interpret
the ingenuity of chatGPT's answers shared on social media, rather than our own
experiences.</p>
<p>Like with horoscopes, with thousands of &quot;genious&quot; chat logs uploaded every day,
those that survive Twitter's algorithm and bubble up are truly stunning.</p>
<h3>iii.</h3>
<p>During last year's crypto currency bull market, a key criticism was (and is)
that the environmental costs of Proof of Work and the economic costs for
Ethereum's Proof of Stake algorithm are too high. Opponents view is, hence,
that web3 isn't faster, cheaper and better than web2, which results in a lack
of meaningful benefits to web3's end user. If transactions are costly and slow,
then why would a &quot;blockchain Twitter&quot; ever outperform a &quot;database Twitter.&quot;</p>
<p>And this line of reasoning is interesting, because it originates from the, to
me, very reasonable idea that the cost of running software is marginal to its
production. Having paid software engineers myself, as well as cloud provider
bills, I can say that, in many projects, one engineer's hour is equal to the
entire project's server bills for a month. Running regular software is
technically free, but creating it isn't.</p>
<p>So then, with crypto currencies' immense security budget spending and Sam
Altman confirming that chatGPT's computation costs are eye-watering, I'm now
wondering if the idea of running software as a marginal cost is history.</p>
<p>While there are no official data on what it must have cost to train OpenAi's
GPT, there are plenty of estimates ball-parking computation costs based on the
model's number of parameters [1, 2, 3, 4].</p>
<table>
<thead>
<tr>
<th>Parameters</th>
<th>Tokens</th>
<th>Cost in USD</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.3B</td>
<td>26B</td>
<td>2k</td>
</tr>
<tr>
<td>2.7B</td>
<td>54B</td>
<td>6k</td>
</tr>
<tr>
<td>6.7B</td>
<td>1.34T</td>
<td>30k</td>
</tr>
<tr>
<td>13B</td>
<td>2.60T</td>
<td>100k</td>
</tr>
<tr>
<td>30B</td>
<td>6.1T</td>
<td>450k</td>
</tr>
<tr>
<td>70B</td>
<td>14T</td>
<td>2.5M</td>
</tr>
<tr>
<td>175B</td>
<td>-</td>
<td>4.6M</td>
</tr>
<tr>
<td>540B</td>
<td>78T</td>
<td>23M</td>
</tr>
<tr>
<td>1T</td>
<td>-</td>
<td>100M (own estimate)</td>
</tr>
</tbody>
</table>
<img alt="LLM production costs" style="display: block; margin: 0 auto; width: 60%;" src="../../assets/images/cost-llms.png" />
<p>Now, by no means do I have a sound-enough understanding of these economics, but
what is notable, is that while parameter amount in models grows
exponentially, so also do costs. When confronting an AI startup's VP about this
trivial (?) observation, his answer was that exponential parameter increase
would naturally yield exponential cost.</p>
<p>And while that answer seemed convincing, if it is squared with the observation
that semiconductor density doubles every two years with chips <em>not</em> doubling in
price, I'm questioning if these transformer models will yield returns on
investment. After all, the software industry's model for production cost has
been that of seeing computation costs as marginal and so I hope investors
understand what they're buying into.</p>
<h3>iv.</h3>
<p>Speaking of marginality, when listening to podcasters discussing the
technology's potential, a stereotypical assessment is that these models already
have a pretty good accuracy, but that with (1) more training, (2) web-browsing
support and (3) capabilities to reference sources, their accuracy problem can
be fixed entirely.</p>
<img alt="What's the fastest marine mammal?" style="display: block; margin: 0 auto; width: 60%;" src="../../assets/images/fastest-mammal.jpg" />
<p>I find remarkable, the psychology that goes into these speculations. Arguably,
the brightest engineers in the world have already spent countless hours and
capital on producing chatGPT. But in terms of thorough thinking it through, it
is unreasonable to identify these &quot;last-mile&quot; features as viable or applicable.
A lot of training has already happened (1), you.com has enabled web-browsing
(2) and referencing sources is frankly a task entirely mutual exclusive to the
transformer architecture (3).</p>
<p>I don't want to say that these qualities can't or won't be achieved, I'm trying
to argue that investors are pricing in this technology's future potential -
with arguably little competence. As a proficent software engineer (with no
meaningful experience in ML), there is no way for me to easily tell whether
implementing source referencing is an achievable feat within the transformer
architecture. And I know how to learn and understand these things fast.</p>
<h3>v.</h3>
<p>The capabilities of these models referencing their sources is in my opinion a
great example to demonstrate investor's delusions. But first, what are
references actually used for?</p>
<p>In this article, in scientific journals and elsewhere, a reference safeguards a
author's claim by pointing to an external resource. In all cases, for the claim
to work, we assume that claim and reference match.</p>
<p>While this is easily glossed over, referencing is a key component of critical
thinking and knowledge production. An academic would happily discuss a paper's
references for hours on end, without even attacking its mere content. It is an
art itself to reference sources and to &quot;take them with a grain of salt.&quot;</p>
<img alt="Who am I?" style="display: block; margin: 0 auto; width: 40%;" src="../../assets/images/who-am-i.png" />
<p>Which makes me particularly surprised that it is considered a &quot;last-mile&quot;
effort by many that these transformer models will eventually mimick this
behavior well.</p>
<p>If academic papers made false claims through reference, interpreted references
wrongly and did so at a significant rate, science would frankly stop working.
The affordance of a reference and its respective claim is that it is very
likely true and that if it isn't, that it's a scandal.</p>
<p>So considering that LLM's are predicting the next token with a low accuracy and
don't have a conceptual understanding of what they &quot;read,&quot; it is far fetched to
assume that they'll make valid claims based on the source material available.</p>
<p>But finally, and this too, seems like a logical error, when wanting to use
chatGPT to summarize verbose texts, a prompter's intention must be to save time
reading it themselves. Which creates a dilemma considering the model's poor
trustworthiness and makes me question the promoted productivity gains.</p>
<h3>vi.</h3>
<p>In K. Popper's &quot;Open Society and its Enemies,&quot; I've come across an well-suited
section for a fallacy also observed in AI enthusiasts. A spiritual
understanding of the neural network is that of compressing knowledge into
weights and biases. A common trope is that these models are small, and that the
knowledge their can regurgitate is big. So talking to chatGPT feels spiritual
and thriling as within this point of view, it could contain hidden nuggets of
knowledge never seen before.</p>
<p>In Popper's book, he draws a line between conventional and physical rules to
make a distinction about a tactic of deception he calls &quot;spiritual naturalism.&quot;
In politics, it is often observed in capitalism proponents who would argue that
capitalism is <em>the</em> system, as men's nature is deeply connected to competition.
But squaring that with the physical law of the earth revolving around the sun
in a observable and unchanging pattern, it becomes clear that &quot;men's nature to
compete&quot; is merely an ephemeral and mutable convention.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">interesting to me how many of the ChatGPT takes are either &quot;this is AGI&quot; (obviously not close, lol) or &quot;this approach can&#39;t really go that much further&quot;.<br><br>trust the exponential. flat looking backwards, vertical looking forwards.</p>&mdash; Sam Altman (@sama) <a href="https://twitter.com/sama/status/1599111626191294464?ref_src=twsrc%5Etfw">December 3, 2022</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p>And while, by no means, I will be capable of arguing a similar remark for those
that credit modern transformer models with uber-natural capabilities, I think
it is clear that their believes aren't anchored in reproducible observations
and that they're likely prey to the same logical fallacy spirital naturalists
are. &quot;Trust in the exponential...&quot;</p>
<h2>References:</h2>
<ul>
<li>1: https://www.mosaicml.com/blog/gpt-3-quality-for-500k</li>
<li>2: https://lambdalabs.com/blog/demystifying-gpt-3</li>
<li>3: https://blog.heim.xyz/palm-training-cost/</li>
<li>4: https://docs.google.com/spreadsheets/d/1AAIebjNsnJj_uKALHbXNfn3_YsT6sHXtCU0q7OIPuc4/edit#gid=0</li>
<li>5: https://help.openai.com/en/articles/6787051-does-chatgpt-remember-what-happened-earlier-in-the-conversation</li>
</ul>

  </body>
</html>

